{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO (You Only Look Once) Implementation for Traffic Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tensorflow version: 2.1.0\nGPU: ['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0']\n"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image\n",
    "import matplotlib.patches as patches\n",
    "import configparser\n",
    "import ast\n",
    "from xml.dom import minidom\n",
    "\n",
    "from src.data_generator import DataGenerator\n",
    "from src.data_preview import DataPreview\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "print('GPU: {}'.format([x.name for x in device_lib.list_local_devices()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting YOLO Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/classes.names') as class_file:\n",
    "    LABELS = class_file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config/params.config')\n",
    "\n",
    "IMAGE_W             = int(config['YOLO']['IMAGE_W'])\n",
    "IMAGE_H             = int(config['YOLO']['IMAGE_H'])\n",
    "GRID_W              = int(config['YOLO']['GRID_W'])\n",
    "GRID_H              = int(config['YOLO']['GRID_H'])\n",
    "BOXES               = int(config['YOLO']['BOXES'])\n",
    "CLASSES             = int(config['YOLO']['CLASSES'])\n",
    "MIN_SCORE           = float(config['YOLO']['MIN_SCORE'])\n",
    "MIN_IOU             = float(config['YOLO']['MIN_IOU'])\n",
    "ANCHORS             = ast.literal_eval(config['YOLO']['ANCHORS'])\n",
    "\n",
    "TRAIN_BATCH_SIZE    = int(config['TRAINING']['TRAIN_BATCH_SIZE'])\n",
    "VAL_BATCH_SIZE      = int(config['TRAINING']['VAL_BATCH_SIZE'])\n",
    "EPOCHS              = int(config['TRAINING']['EPOCHS'])\n",
    "\n",
    "IMAGE_FOLDER        = config['DIR']['IMAGES']\n",
    "ANNOTATION_FOLDER   = config['DIR']['ANNOTATIONS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training/validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<PrefetchDataset shapes: ((None, 512, 512, 3), (None, 16, 16, 5, 12)), types: (tf.float32, tf.float32)>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset = DataGenerator.generate_tf_dataset(\n",
    "    ANNOTATION_FOLDER,\n",
    "    IMAGE_FOLDER,\n",
    "    LABELS,\n",
    "    BOXES,\n",
    "    TRAIN_BATCH_SIZE,\n",
    "    (GRID_W, GRID_H),\n",
    "    (IMAGE_W, IMAGE_H))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = DataGenerator.augment_dataset(\n",
    "#     dataset,\n",
    "#     (IMAGE_W, IMAGE_H))\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 512, 512, 32)      864       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 512, 512, 32)      128       \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 512, 512, 32)      0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 256, 256, 32)      0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 256, 256, 64)      18432     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 256, 256, 64)      256       \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 256, 256, 64)      0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 128, 128, 128)     73728     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 128, 128, 128)     512       \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 128)     0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 128, 128, 64)      73728     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 128, 128, 64)      256       \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 128, 128, 128)     73728     \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 128, 128, 128)     512       \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 128)     0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 64, 64, 256)       294912    \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 64, 64, 256)       1024      \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 256)       0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 64, 64, 128)       294912    \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 64, 64, 128)       512       \n_________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 64, 64, 256)       294912    \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 64, 64, 256)       1024      \n_________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 256)       0         \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 32, 32, 512)       1179648   \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 32, 32, 512)       2048      \n_________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 512)       0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 32, 32, 256)       1179648   \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 32, 32, 256)       1024      \n_________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)    (None, 32, 32, 256)       0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 32, 32, 512)       1179648   \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 32, 32, 512)       2048      \n_________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 512)       0         \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 16, 16, 1024)      4718592   \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)   (None, 16, 16, 1024)      0         \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 16, 16, 512)       4718592   \n_________________________________________________________________\nbatch_normalization_12 (Batc (None, 16, 16, 512)       2048      \n_________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)   (None, 16, 16, 512)       0         \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 16, 16, 1024)      4718592   \n_________________________________________________________________\nbatch_normalization_13 (Batc (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 1024)      0         \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 16, 16, 512)       4718592   \n_________________________________________________________________\nbatch_normalization_14 (Batc (None, 16, 16, 512)       2048      \n_________________________________________________________________\nleaky_re_lu_14 (LeakyReLU)   (None, 16, 16, 512)       0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 16, 16, 1024)      4718592   \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nleaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 1024)      0         \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 16, 16, 1024)      9437184   \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 16, 16, 1024)      4096      \n_________________________________________________________________\nleaky_re_lu_16 (LeakyReLU)   (None, 16, 16, 1024)      0         \n_________________________________________________________________\ndropout (Dropout)            (None, 16, 16, 1024)      0         \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 16, 16, 60)        61500     \n_________________________________________________________________\nreshape (Reshape)            (None, 16, 16, 5, 12)     0         \n=================================================================\nTotal params: 37,785,628\nTrainable params: 37,770,716\nNon-trainable params: 14,912\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "\n",
    "model = models.Sequential([\n",
    "    # (512, 512, 3) -> conv_0 + norm_0 + relu_0 + max_pool_0 -> (256, 256, 32)\n",
    "    layers.Conv2D(32, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(512, 512, 3)),\n",
    "    layers.BatchNormalization(input_shape=(512, 512, 32)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(512, 512, 32)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), input_shape=(512, 512, 32)),\n",
    "\n",
    "\n",
    "    # (256, 256, 32) -> conv_1 + norm_1 + relu_1 + max_pool_1 -> (128, 128, 64)\n",
    "    layers.Conv2D(64, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(256, 256, 32)),\n",
    "    layers.BatchNormalization(input_shape=(256, 256, 64)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(256, 256, 64)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), input_shape=(256, 256, 64)),\n",
    "\n",
    "\n",
    "    # (128, 128, 64) -> conv_2 + norm_2 + relu_2 -> (128, 128, 128)\n",
    "    layers.Conv2D(128, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(128, 128, 128)),\n",
    "    layers.BatchNormalization(input_shape=(128, 128, 128)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(128, 128, 128)),\n",
    "\n",
    "    # (128, 128, 128) -> conv_3 + norm_3 + relu_3 -> (128, 128, 64)\n",
    "    layers.Conv2D(64, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(128, 128, 32)),\n",
    "    layers.BatchNormalization(input_shape=(128, 128, 64)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(128, 128, 64)),\n",
    "\n",
    "    # (128, 128, 64) -> conv_4 + norm_4 + relu_4 + max_pool_2 -> (64, 64, 128)\n",
    "    layers.Conv2D(128, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(128, 128, 64)),\n",
    "    layers.BatchNormalization(input_shape=(128, 128, 128)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(128, 128, 128)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), input_shape=(128, 128, 128)),\n",
    "\n",
    "\n",
    "    # (64, 64, 128) -> conv_5 + norm_5 + relu_5 -> (64, 64, 256)\n",
    "    layers.Conv2D(256, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(64, 64, 128)),\n",
    "    layers.BatchNormalization(input_shape=(64, 64, 256)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(64, 64, 256)),\n",
    "\n",
    "    # (64, 64, 256) -> conv_6 + norm_6 + relu_6 -> (64, 64, 128)\n",
    "    layers.Conv2D(128, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(64, 64, 256)),\n",
    "    layers.BatchNormalization(input_shape=(64, 64, 128)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(64, 64, 128)),\n",
    "\n",
    "    # (64, 64, 128) -> conv_7 + norm_7 + relu_7 + max_pool_3 -> (32, 32, 256)\n",
    "    layers.Conv2D(256, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(64, 64, 128)),\n",
    "    layers.BatchNormalization(input_shape=(64, 64, 256)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(64, 64, 256)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), input_shape=(64, 64, 256)),\n",
    "\n",
    "\n",
    "    # (32, 32, 256) -> conv_8 + norm_8 + relu_8 -> (32, 32, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32, 32, 256)),\n",
    "    layers.BatchNormalization(input_shape=(32, 32, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(32, 32, 512)),\n",
    "\n",
    "    # (32, 32, 512) -> conv_9 + norm_9 + relu_9 -> (32, 32, 256)\n",
    "    layers.Conv2D(256, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32, 32, 512)),\n",
    "    layers.BatchNormalization(input_shape=(32, 32, 256)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(32, 32, 256)),\n",
    "\n",
    "    # (32, 32, 256) -> conv_10 + norm_10 + relu_10 -> (32, 32, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32, 32, 256)),\n",
    "    layers.BatchNormalization(input_shape=(32, 32, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(32, 32, 512)),\n",
    "\n",
    "    # (32, 32, 512) -> conv_11 + norm_11 + relu_11 -> (32, 32, 256)\n",
    "    layers.Conv2D(256, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32, 32, 512)),\n",
    "    layers.BatchNormalization(input_shape=(32, 32, 256)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(32, 32, 256)),\n",
    "\n",
    "    # (32, 32, 256) -> conv_12 + norm_12 + relu_12 + max_pool_4 -> (16, 16, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(32, 32, 256)),\n",
    "    layers.BatchNormalization(input_shape=(32, 32, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(32, 32, 512)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), input_shape=(32, 32, 512)),\n",
    "\n",
    "\n",
    "    # (16, 16, 512) -> conv_13 + norm_13 + relu_13 -> (16, 16, 1024)\n",
    "    layers.Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 512)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 1024)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 1024)),\n",
    "\n",
    "    # (16, 16, 1024) -> conv_14 + norm_14 + relu_14 -> (16, 16, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 1024)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 512)),\n",
    "\n",
    "    # (16, 16, 512) -> conv_15 + norm_15 + relu_15 -> (16, 16, 1024)\n",
    "    layers.Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 512)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 1024)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 1024)),\n",
    "\n",
    "    # (16, 16, 1024) -> conv_16 + norm_16 + relu_16 -> (16, 16, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 1024)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 512)),\n",
    "\n",
    "    # (16, 16, 512) -> conv_17 + norm_17 + relu_17 -> (16, 16, 1024)\n",
    "    layers.Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 512)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 1024)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 1024)),\n",
    "\n",
    "    # (16, 16, 1024) -> conv_18 + norm_18 + relu_18 -> (16, 16, 512)\n",
    "    layers.Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 1024)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 512)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 512)),\n",
    "\n",
    "    # (16, 16, 512) -> conv_19 + norm_19 + relu_19 -> (16, 16, 1024)\n",
    "    layers.Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 512)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 1024)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 1024)),\n",
    "\n",
    "\n",
    "    # (16, 16, 1024) -> conv_20 + norm_20 + relu_20 + dropout -> (16, 16, 1024)\n",
    "    layers.Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(16, 16, 1024)),\n",
    "    layers.BatchNormalization(input_shape=(16, 16, 1024)),\n",
    "    layers.LeakyReLU(alpha=0.1, input_shape=(16, 16, 1024)),\n",
    "    layers.Dropout(0.3, input_shape=(16, 16, 1024)),\n",
    "\n",
    "    # (16, 16, 1024) -> output_layer -> (16, 16, 5, 12)\n",
    "    layers.Conv2D(BOXES * (5 + CLASSES), (1,1), strides=(1,1), padding='same'),\n",
    "    layers.Reshape((GRID_W, GRID_H, BOXES, 5 + CLASSES))\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tensor(\"reshape/Identity:0\", shape=(None, 16, 16, 5, 12), dtype=float32) Tensor(\"reshape_target:0\", shape=(None, None, None, None, None), dtype=float32)\nTensor(\"loss/reshape_loss/Square:0\", shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "from src.model_training import ModelTraining\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "    loss=ModelTraining.loss,\n",
    "    metrics=[ModelTraining.IOU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 1 steps\nTensor(\"loss/reshape_loss/Square:0\", shape=(), dtype=float32)\nTensor(\"sequential/reshape/Reshape:0\", shape=(None, 16, 16, 5, 12), dtype=float32) Tensor(\"IteratorGetNext:1\", shape=(None, 16, 16, 5, 12), dtype=float32)\nTensor(\"loss/reshape_loss/Square:0\", shape=(), dtype=float32)\nTensor(\"sequential/reshape/Reshape:0\", shape=(None, 16, 16, 5, 12), dtype=float32) Tensor(\"IteratorGetNext:1\", shape=(None, 16, 16, 5, 12), dtype=float32)\n1/1 [==============================] - 6s 6s/step - loss: 0.1310 - IOU: nan\n"
    }
   ],
   "source": [
    "detector = model.fit(dataset, epochs=EPOCHS, steps_per_epoch=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /media/data/Projects/programming/traffic-brain/feature/classification/classification/env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: models/tb__2020_06_03__17_20_35/assets\n"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "model.save('models/{}/'.format(datetime.now().strftime('tb__%Y_%m_%d__%H_%M_%S')))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python36964bitenvvenva49043241aac44daa839b20da5071fef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}